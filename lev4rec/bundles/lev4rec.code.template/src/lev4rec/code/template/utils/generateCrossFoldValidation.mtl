[comment encoding = UTF-8 /]
[module generateCrossFoldValidation('https://it.disim.univaq/lowcode')]
[import lev4rec::code::template::utils::generatePreprocessingTechnique/]
[import lev4rec::code::template::utils::generateRecommenderSystem/]

[template public generateCrossFoldValidation(recSys : RSModel)]
[for (val : ValidationTechnique | recSys.evaluation.validationTechnique)]
[if (val.oclIsTypeOf(lowcoders::CrossValidation))]	
[if (recSys.recommendationSystem.generator = PyLibType::SKLEARN)]
	X,y = load_dataset()
	from sklearn.model_selection import  KFold
    n_splits=[val.oclAsType(lowcoders::CrossValidation).numberOfFold/]
    kf = KFold(n_splits = n_splits)	
	prec_all = 0
	acc_all = 0
	rec_all = 0
	f1_all = 0	
	list_metrics=['['/][']'/]
	for  train, test in kf.split(X):
		X_split, X_test, y_split, y_test = X['['/]train[']'/],X['['/]test[']'/], y['['/]train[']'/], y['['/]test[']'/]	
		sc=set_preprocessing()
		list_train=['['/][']'/]
        list_test =['['/][']'/]
        for x in X_split.tolist():
            list_train.append(str(x))
        for t in X_test.tolist():
            list_test.append(str(t))
        X_train=sc.fit_transform(list_train)
        X_test=sc.transform(list_test)     
		clf= algorithm_settings()
        clf.fit(X_train, y_split)        
        y_pred= clf.predict(X_test)				
	[for (metric : Metric | val.eContainer().oclAsType(lowcoders::Evaluation).metrics) ]
	[if (metric = Metric::PRECISION) ]
		from sklearn.metrics import precision_score
		precision = precision_score(y_pred, y_test, average=None)
		prec_all = prec_all + sum(precision)/len(precision)	
	[/if]	
	[if (metric = Metric::RECALL) ]
		from sklearn.metrics import recall_score
		recall = recall_score(y_pred, y_test, average=None)
		rec_all = rec_all + sum(recall)/len(recall)
	[/if]
	[if (metric = Metric::F1_MEASURE) ]
		from sklearn.metrics import f1_score
		f1 = f1_score(y_pred, y_test, average=None)
		f1_all = f1_all + sum(f1)/len(f1)
	[/if]	
	[/for]
	prec_all=(prec_all /n_splits)
    rec_all=(rec_all /n_splits)
    f1_all=(f1_all /n_splits)  
 
    list_metrics.append(prec_all)
    list_metrics.append(rec_all)
    list_metrics.append(f1_all)

	return list_metrics
    
[/if]
[/if]

[if (recSys.recommendationSystem.generator = PyLibType::SURPRISE)]

from surprise.model_selection import KFold
from collections import defaultdict
kf = KFold(n_splits=n_splits)
#algo = SVD()
[if (recSys.dataset.preprocessing -> size()>0)]
		[for (pr : PreprocessingTechnique | recSys.dataset.preprocessing)]
	[generatePreprocessingTechnique(pr, recSys)/]
		[/for]
	[/if]



	algo=algorithm_settings()
	threshold = 3.5
	k=10
	for trainset, testset in kf.split(data):
	    algo.fit(trainset)
	    predictions = algo.test(testset)
	    #precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)

[for (metric : Metric | val.eContainer().oclAsType(lowcoders::Evaluation).metrics) ]
	[if (metric = Metric::PRECISION) ]
	user_est_true = defaultdict(list)
    for uid, _, true_r, est, _ in predictions:
        user_est_true['['/]uid[']'/].append((est, true_r))

    precisions = dict()
	recalls = dict()
    for uid, user_ratings in user_est_true.items():
        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)

        # Number of recommended items in top k
        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings['['/]:k[']'/])
        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))
                              for (est, true_r) in user_ratings['['/]:k[']'/])
        precisions['['/]uid[']'/] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0

	[/if]	
	[if (metric = Metric::RECALL) ]
		recalls['['/]uid[']'/] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0
	[/if]

	[/for]
	precision= sum(prec for prec in precisions.values()) / len(precisions)
	recall =sum(rec for rec in recalls.values()) / len(recalls)
    # Precision and recall can then be averaged over all users
    print(precision)
    print(recall)
	[for (metric : Metric | val.eContainer().oclAsType(lowcoders::Evaluation).metrics) ]
	[if (metric = Metric::F1_MEASURE) ]
	f1_measure=(2*precision* recall) / (recall + precision)
	print(f1_measure)
	[/if]	

	[/for]


[/if]
[/for]
[/template]
